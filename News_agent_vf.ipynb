{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14814735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from newsapi import NewsApiClient\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c712ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Configuration for the news attribution pipeline\"\"\"\n",
    "    newsapi_key: str\n",
    "    groq_api_key: str\n",
    "    \n",
    "    # Pipeline parameters\n",
    "    abnormal_return_threshold: float = 0.05 # Â±5%\n",
    "    lookback_days: int = 7\n",
    "    relevance_thresholds: List[float] = None\n",
    "    sentiment_positive_threshold: float = 0.2\n",
    "    sentiment_negative_threshold: float = -0.2\n",
    "    llm_temperature: float = 0.3\n",
    "    llm_max_tokens: int = 1000\n",
    "    max_articles_per_source: int = 20\n",
    "    article_content_length: int = 2000\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.relevance_thresholds is None:\n",
    "            self.relevance_thresholds = [0.70, 0.50, 0.25]\n",
    "\n",
    "\n",
    "NEWS_CATALYSTS = [\n",
    "    \"Earnings Report\", \"Guidance/Forecast\", \"M&A\", \"Share Buyback\",\n",
    "    \"Analyst Upgrade/Downgrade\", \"Dividend Changes\", \"FDA/Clinical Trial\",\n",
    "    \"CEO/CFO Change\", \"Stock Split\", \"Index Rebalancing\",\n",
    "    \"Activist Investor\", \"Short Squeeze/Meme Hype\", \"Insider Transactions\",\n",
    "    \"Litigation/Regulatory\", \"Secondary Offering\", \"Cybersecurity Breach\",\n",
    "    \"Trade/Tariff News\", \"Economic Data/Fed\", \"Contract Wins/Product Launch\",\n",
    "    \"Patent/Licensing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ab2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StockMove:\n",
    "    \"\"\"Represents an abnormal stock price movement\"\"\"\n",
    "    ticker: str\n",
    "    date: datetime\n",
    "    percent_change: float\n",
    "    close: float\n",
    "    volume: int\n",
    "\n",
    "@dataclass\n",
    "class NewsArticle:\n",
    "    \"\"\"Represents a news article\"\"\"\n",
    "    title: str\n",
    "    url: str\n",
    "    source: str\n",
    "    published: str\n",
    "    description: str\n",
    "    content: str = \"\"\n",
    "    relevance_score: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class AnalysisResult:\n",
    "    \"\"\"Results of news analysis for a stock move\"\"\"\n",
    "    ticker: str\n",
    "    date: datetime\n",
    "    percent_change: float\n",
    "    num_articles: int\n",
    "    sentiment_score: float\n",
    "    sentiment_label: str\n",
    "    summary: str\n",
    "    explanation: str\n",
    "    catalyst: str = \"\"\n",
    "\n",
    "class PriceDetector:\n",
    "    \"\"\"Detects abnormal stock price movements\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def get_tickers(self) -> List[str]:\n",
    "        \"\"\"Get list of tickers to monitor\"\"\"\n",
    "        return [\n",
    "            'INTC', 'ADBE', 'CRM', 'NEE', 'LLY', 'VZ', 'GOOGL', 'NFLX', \n",
    "            'AMD', 'PYPL', 'CSCO', 'TMO', 'PG'\n",
    "        ]\n",
    "    \n",
    "    def detect_abnormal_returns(self, tickers: List[str] = None) -> List[StockMove]:\n",
    "        \"\"\"Identify stocks with abnormal daily returns\"\"\"\n",
    "        if tickers is None:\n",
    "            tickers = self.get_tickers()\n",
    "        \n",
    "        stock_moves = []\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=self.config.lookback_days + 5)\n",
    "        \n",
    "        print(f\"[PriceDetector] Scanning {len(tickers)} tickers for abnormal returns...\")\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                stock = yf.Ticker(ticker)\n",
    "                hist = stock.history(start=start_date, end=end_date)\n",
    "                \n",
    "                if len(hist) < 2:\n",
    "                    continue\n",
    "                \n",
    "                hist['pct_change'] = hist['Close'].pct_change() * 100\n",
    "                hist = hist.dropna(subset=['pct_change'])\n",
    "                \n",
    "                if len(hist) == 0:\n",
    "                    continue\n",
    "                \n",
    "                recent_moves = hist.tail(self.config.lookback_days)\n",
    "                \n",
    "                for date, row in recent_moves.iterrows():\n",
    "                    if abs(row['pct_change']) >= self.config.abnormal_return_threshold * 100:\n",
    "                        stock_moves.append(StockMove(\n",
    "                            ticker=ticker,\n",
    "                            date=date,\n",
    "                            percent_change=row['pct_change'],\n",
    "                            close=row['Close'],\n",
    "                            volume=int(row['Volume']) if pd.notna(row['Volume']) else 0\n",
    "                        ))\n",
    "            except Exception as e:\n",
    "                print(f\"[PriceDetector] Error processing {ticker}: {str(e)}\")\n",
    "        \n",
    "        # Sort by absolute magnitude\n",
    "        stock_moves.sort(key=lambda x: abs(x.percent_change), reverse=True)\n",
    "        \n",
    "        print(f\"[PriceDetector] Found {len(stock_moves)} abnormal movements\")\n",
    "        return stock_moves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43340a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsRetriever:\n",
    "    \"\"\"Retrieves news from multiple sources\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        #self.newsapi = NewsApiClient(api_key=config.newsapi_key)\n",
    "    \n",
    "    def get_company_name(self, ticker: str) -> str:\n",
    "        \"\"\"Get full company name from ticker\"\"\"\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            return stock.info.get('longName', ticker)\n",
    "        except:\n",
    "            return ticker\n",
    "    \n",
    "    def fetch_google_news(self, ticker: str, date: datetime) -> List[NewsArticle]:\n",
    "        \"\"\"Fetch news from Google News RSS\"\"\"\n",
    "        articles = []\n",
    "        try:\n",
    "            date_str = date.strftime('%Y-%m-%d') if isinstance(date, datetime) else str(date).split()[0]\n",
    "            target_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "            after_date = (target_date - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "            before_date = (target_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "            \n",
    "            query = f\"{ticker} stock after:{after_date} before:{before_date}\"\n",
    "            url = f\"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=en-US&gl=US&ceid=US:en\"\n",
    "            \n",
    "            feed = feedparser.parse(url)\n",
    "            \n",
    "            for entry in feed.entries[:15]:\n",
    "                articles.append(NewsArticle(\n",
    "                    title=entry.title,\n",
    "                    url=entry.link,\n",
    "                    source='Google News',\n",
    "                    published=entry.get('published', ''),\n",
    "                    description=entry.get('summary', '')\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            print(f\"[NewsRetriever] Google News error for {ticker}: {str(e)}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def fetch_yahoo_finance(self, ticker: str) -> List[NewsArticle]:\n",
    "        \"\"\"Fetch news from Yahoo Finance RSS\"\"\"\n",
    "        articles = []\n",
    "        try:\n",
    "            url = f\"https://finance.yahoo.com/rss/headline?s={ticker}\"\n",
    "            feed = feedparser.parse(url)\n",
    "            \n",
    "            for entry in feed.entries[:10]:\n",
    "                articles.append(NewsArticle(\n",
    "                    title=entry.title,\n",
    "                    url=entry.link,\n",
    "                    source='Yahoo Finance',\n",
    "                    published=entry.get('published', ''),\n",
    "                    description=entry.get('summary', '')\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            print(f\"[NewsRetriever] Yahoo Finance error for {ticker}: {str(e)}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def scrape_article_content(self, url: str) -> str:\n",
    "        \"\"\"Scrape full article text\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            clean_url = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            response = requests.get(clean_url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            text = ' '.join([p.get_text() for p in paragraphs])\n",
    "            return text[:self.config.article_content_length]\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def deduplicate_articles(self, articles: List[NewsArticle]) -> List[NewsArticle]:\n",
    "        \"\"\"Remove duplicate articles\"\"\"\n",
    "        seen = set()\n",
    "        unique_articles = []\n",
    "        \n",
    "        for article in articles:\n",
    "            norm_title = re.sub(r'\\s+', ' ', article.title.lower().strip())\n",
    "            parsed = urlparse(article.url)\n",
    "            norm_url = f\"{parsed.netloc}{parsed.path}\"\n",
    "            key = (norm_title, norm_url)\n",
    "            \n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_articles.append(article)\n",
    "        \n",
    "        return unique_articles\n",
    "    \n",
    "    def retrieve_news(self, ticker: str, date: datetime) -> List[NewsArticle]:\n",
    "        \"\"\"Aggregate news from all sources\"\"\"\n",
    "        company_name = self.get_company_name(ticker)\n",
    "        \n",
    "        all_articles = []\n",
    "        #all_articles.extend(self.fetch_newsapi_articles(ticker, company_name, date))\n",
    "        all_articles.extend(self.fetch_google_news(ticker, date))\n",
    "        all_articles.extend(self.fetch_yahoo_finance(ticker))\n",
    "        \n",
    "        unique_articles = self.deduplicate_articles(all_articles)\n",
    "        \n",
    "        # Scrape content\n",
    "        for article in unique_articles:\n",
    "            article.content = self.scrape_article_content(article.url)\n",
    "\n",
    "        return unique_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a8f6cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMInterface:\n",
    "    \"\"\"Interface for LLM API calls\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.api_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    \n",
    "    def call_with_retry(self, prompt: str, max_retries: int = 3) -> Optional[str]:\n",
    "        \"\"\"Call LLM with retry logic\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.api_url,\n",
    "                    headers={\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                        \"Authorization\": f\"Bearer {self.config.groq_api_key}\"\n",
    "                    },\n",
    "                    json={\n",
    "                        \"model\": \"llama-3.1-8b-instant\",\n",
    "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                        \"temperature\": self.config.llm_temperature,\n",
    "                        \"max_tokens\": self.config.llm_max_tokens\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    return result['choices'][0]['message']['content']\n",
    "                elif response.status_code == 429:\n",
    "                    wait_time = min(60, 2 ** attempt * 10)\n",
    "                    print(f\"[LLMInterface] Rate limit hit, waiting {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise Exception(f\"API error: {response.status_code}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):\n",
    "                    wait_time = min(60, 2 ** attempt * 10)\n",
    "                    print(f\"[LLMInterface] Rate limit (attempt {attempt+1}/{max_retries}), waiting {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"[LLMInterface] Error: {str(e)}\")\n",
    "                    return None\n",
    "        \n",
    "        print(f\"[LLMInterface] Max retries reached\")\n",
    "        return None\n",
    "\n",
    "class RelevanceScorer:\n",
    "    \"\"\"Scores article relevance using LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.llm = LLMInterface(config)\n",
    "    \n",
    "    def score_article(self, article: NewsArticle, ticker: str, company_name: str, price_change: float) -> float:\n",
    "        \"\"\"Score a single article's relevance\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"You are a financial analyst. Rate the relevance of this news article to explain the stock price movement.\n",
    "\n",
    "Company: {company_name} ({ticker})\n",
    "Price Change: {price_change:.2f}%\n",
    "Article Title: {article.title}\n",
    "Article Content: {article.content[:1000]}\n",
    "\n",
    "On a scale of 0.0 to 1.0, how relevant is this article to explaining the stock price movement?\n",
    "- 1.0: Directly explains the price movement with company-specific news\n",
    "- 0.5: Mentions the company but not directly related to price movement\n",
    "- 0.0: Not related to the company or its stock price\n",
    "\n",
    "Respond with ONLY a number between 0.0 and 1.0.\"\"\"\n",
    "\n",
    "            response_text = self.llm.call_with_retry(prompt)\n",
    "            \n",
    "            if response_text is None:\n",
    "                return 0.0\n",
    "            \n",
    "            score_match = re.search(r'0?\\.\\d+|[01]\\.0|^[01]$', response_text.strip())\n",
    "            \n",
    "            if score_match:\n",
    "                score = float(score_match.group())\n",
    "                return score\n",
    "            \n",
    "            return 0.0\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[RelevanceScorer] Error: {str(e)}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def filter_relevant_articles(self, articles: List[NewsArticle], ticker: str, \n",
    "                                 company_name: str, price_change: float) -> List[NewsArticle]:\n",
    "        \"\"\"Filter articles by relevance score\"\"\"\n",
    "        print(f\"[RelevanceScorer] Scoring {len(articles)} articles...\")\n",
    "        \n",
    "        scored_articles = []\n",
    "        \n",
    "        for i, article in enumerate(articles):\n",
    "            score = self.score_article(article, ticker, company_name, price_change)\n",
    "            article.relevance_score = score\n",
    "            scored_articles.append(article)\n",
    "            \n",
    "            if i < len(articles) - 1:\n",
    "                time.sleep(2)  # Rate limiting\n",
    "        \n",
    "        # Try thresholds in descending order\n",
    "        for threshold in self.config.relevance_thresholds:\n",
    "            relevant = [a for a in scored_articles if a.relevance_score >= threshold]\n",
    "            if relevant:\n",
    "                print(f\"[RelevanceScorer] {len(relevant)} articles passed {threshold} threshold\")\n",
    "                return relevant\n",
    "        \n",
    "        print(f\"[RelevanceScorer] No articles met minimum relevance criteria\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42c79601",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSummarizer:\n",
    "    \"\"\"Generates structured summaries using LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.llm = LLMInterface(config)\n",
    "    \n",
    "    def summarize(self, articles: List[NewsArticle], ticker: str, \n",
    "                  company_name: str, price_change: float) -> str:\n",
    "        \"\"\"Generate structured summary\"\"\"\n",
    "        try:\n",
    "            combined_content = \"\\n\\n\".join([\n",
    "                f\"Article {i+1}: {a.title}\\n{a.content[:500]}\"\n",
    "                for i, a in enumerate(articles[:5])\n",
    "            ])\n",
    "            \n",
    "            catalyst_list = \"\\n\".join([f\"{i+1}. {cat}\" for i, cat in enumerate(NEWS_CATALYSTS)])\n",
    "            \n",
    "            prompt = f\"\"\"You are a financial analyst. Analyze these news articles about {company_name} ({ticker}).\n",
    "\n",
    "Stock Price Change: {price_change:.2f}%\n",
    "\n",
    "News Articles:\n",
    "{combined_content}\n",
    "\n",
    "News Catalyst Categories:\n",
    "{catalyst_list}\n",
    "\n",
    "Task:\n",
    "1. Extract up to 3 key sentences that explain the price movement\n",
    "2. Identify which catalyst category(ies) apply (use numbers from list)\n",
    "3. Write a single paragraph (3-5 sentences) explaining the likely cause and overall sentiment\n",
    "\n",
    "Format your response as:\n",
    "KEY POINTS:\n",
    "- [point 1]\n",
    "- [point 2]\n",
    "- [point 3]\n",
    "\n",
    "CATALYST: [number(s)]\n",
    "\n",
    "EXPLANATION:\n",
    "[single paragraph]\"\"\"\n",
    "\n",
    "            response_text = self.llm.call_with_retry(prompt)\n",
    "            \n",
    "            if response_text is None:\n",
    "                return \"Summary generation failed due to API limits\"\n",
    "            \n",
    "            return response_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[NewsSummarizer] Error: {str(e)}\")\n",
    "            return \"Summary generation failed\"\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"Analyzes sentiment using FinBERT\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "        self.model.eval()\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Analyze sentiment of text\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                               max_length=512, padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            pos_prob = probs[0][0].item()\n",
    "            neg_prob = probs[0][1].item()\n",
    "            neu_prob = probs[0][2].item()\n",
    "            \n",
    "            sentiment_score = pos_prob - neg_prob\n",
    "        \n",
    "        return {\n",
    "            'positive_prob': pos_prob,\n",
    "            'negative_prob': neg_prob,\n",
    "            'neutral_prob': neu_prob,\n",
    "            'sentiment_score': sentiment_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da557cce",
   "metadata": {},
   "source": [
    "# MAIN PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72c3fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsAttributionPipeline:\n",
    "    \"\"\"MAIN PIP\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.price_detector = PriceDetector(config)\n",
    "        self.news_retriever = NewsRetriever(config)\n",
    "        self.relevance_scorer = RelevanceScorer(config)\n",
    "        self.summarizer = NewsSummarizer(config)\n",
    "        self.sentiment_analyzer = SentimentAnalyzer()\n",
    "    \n",
    "    def analyze_stock_move(self, stock_move: StockMove) -> AnalysisResult:\n",
    "        \"\"\"Analyze a single stock movement\"\"\"\n",
    "        print(f\"\\n[Pipeline] Processing: {stock_move.ticker} | {stock_move.percent_change:.2f}% | {stock_move.date}\")\n",
    "        \n",
    "        company_name = self.news_retriever.get_company_name(stock_move.ticker)\n",
    "        \n",
    "        # Retrieve news\n",
    "        articles = self.news_retriever.retrieve_news(stock_move.ticker, stock_move.date)\n",
    "        \n",
    "        if not articles:\n",
    "            return AnalysisResult(\n",
    "                ticker=stock_move.ticker,\n",
    "                date=stock_move.date,\n",
    "                percent_change=stock_move.percent_change,\n",
    "                num_articles=0,\n",
    "                sentiment_score=0.0,\n",
    "                sentiment_label='No Data',\n",
    "                summary='No news articles found',\n",
    "                explanation='N/A'\n",
    "            )\n",
    "        \n",
    "        # Filter relevant articles\n",
    "        relevant_articles = self.relevance_scorer.filter_relevant_articles(\n",
    "            articles, stock_move.ticker, company_name, stock_move.percent_change\n",
    "        )\n",
    "        \n",
    "        if not relevant_articles:\n",
    "            return AnalysisResult(\n",
    "                ticker=stock_move.ticker,\n",
    "                date=stock_move.date,\n",
    "                percent_change=stock_move.percent_change,\n",
    "                num_articles=len(articles),\n",
    "                sentiment_score=0.0,\n",
    "                sentiment_label='No Relevant News',\n",
    "                summary='No articles met relevancy criteria',\n",
    "                explanation='N/A'\n",
    "            )\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = self.summarizer.summarize(\n",
    "            relevant_articles, stock_move.ticker, company_name, stock_move.percent_change\n",
    "        )\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        sentiment_result = self.sentiment_analyzer.analyze(summary)\n",
    "        sentiment_score = sentiment_result['sentiment_score']\n",
    "        \n",
    "        if sentiment_score > self.config.sentiment_positive_threshold:\n",
    "            sentiment_label = 'Positive'\n",
    "        elif sentiment_score < self.config.sentiment_negative_threshold:\n",
    "            sentiment_label = 'Negative'\n",
    "        else:\n",
    "            sentiment_label = 'Neutral'\n",
    "        \n",
    "        print(f\"[Pipeline] Sentiment Score: {sentiment_score:.3f} ({sentiment_label})\")\n",
    "        \n",
    "        return AnalysisResult(\n",
    "            ticker=stock_move.ticker,\n",
    "            date=stock_move.date,\n",
    "            percent_change=stock_move.percent_change,\n",
    "            num_articles=len(relevant_articles),\n",
    "            sentiment_score=sentiment_score,\n",
    "            sentiment_label=sentiment_label,\n",
    "            summary=summary,\n",
    "            explanation=summary\n",
    "        )\n",
    "    \n",
    "    def run(self) -> List[AnalysisResult]:\n",
    "        \"\"\"Run full pipeline\"\"\"\n",
    "        \n",
    "        stock_moves = self.price_detector.detect_abnormal_returns()\n",
    "        \n",
    "        if not stock_moves:\n",
    "            print(\"[Pipeline] No abnormal stock movements detected\")\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        for stock_move in stock_moves:\n",
    "            result = self.analyze_stock_move(stock_move)\n",
    "            results.append(result)\n",
    "        \n",
    "    \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb1d594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportGenerator:\n",
    "    \"\"\"Generates formatted reports\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_text_report(results: List[AnalysisResult]) -> str:\n",
    "        \"\"\"Generate formatted text report\"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"=\"*80)\n",
    "        lines.append(\"AUTOMATED NEWS ATTRIBUTION REPORT\")\n",
    "        lines.append(\"=\"*80)\n",
    "        lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        lines.append(f\"Total Stocks Analyzed: {len(results)}\")\n",
    "        lines.append(\"=\"*80)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            lines.append(f\"\\n{'='*80}\")\n",
    "            lines.append(f\"STOCK #{i}: {result.ticker}\")\n",
    "            lines.append(f\"{'='*80}\")\n",
    "            lines.append(f\"Date: {result.date.strftime('%Y-%m-%d')}\")\n",
    "            lines.append(f\"Price Change: {result.percent_change:+.2f}%\")\n",
    "            lines.append(f\"Articles Found: {result.num_articles}\")\n",
    "            lines.append(f\"Sentiment: {result.sentiment_label} (Score: {result.sentiment_score:+.3f})\")\n",
    "            lines.append(f\"\\nSUMMARY:\")\n",
    "            lines.append(\"-\"*80)\n",
    "            lines.append(result.summary)\n",
    "            lines.append(\"-\"*80)\n",
    "        \n",
    "        lines.append(f\"\\n{'='*80}\")\n",
    "        lines.append(\"END OF REPORT\")\n",
    "        lines.append(\"=\"*80)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_report(results: List[AnalysisResult], filename: str = \"news_attribution_report.txt\"):\n",
    "        \"\"\"Save report to text file\"\"\"\n",
    "        report = ReportGenerator.generate_text_report(results)\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"[ReportGenerator] Report saved to {filename}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_csv(results: List[AnalysisResult], filename: str = \"results.csv\"):\n",
    "        \"\"\"Save results to CSV\"\"\"\n",
    "        data = []\n",
    "        for result in results:\n",
    "            data.append({\n",
    "                'ticker': result.ticker,\n",
    "                'date': result.date.strftime('%Y-%m-%d'),\n",
    "                'percent_change': result.percent_change,\n",
    "                'num_articles': result.num_articles,\n",
    "                'sentiment_score': result.sentiment_score,\n",
    "                'sentiment_label': result.sentiment_label,\n",
    "                'summary': result.summary\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"[ReportGenerator] CSV saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82e5b57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PriceDetector] Scanning 13 tickers for abnormal returns...\n",
      "[PriceDetector] Found 2 abnormal movements\n",
      "\n",
      "[Pipeline] Processing: ADBE | 5.33% | 2025-12-05 00:00:00-05:00\n",
      "[RelevanceScorer] Scoring 25 articles...\n",
      "[RelevanceScorer] 12 articles passed 0.7 threshold\n",
      "[Pipeline] Sentiment Score: 0.925 (Positive)\n",
      "\n",
      "[Pipeline] Processing: CRM | 5.30% | 2025-12-05 00:00:00-05:00\n",
      "[RelevanceScorer] Scoring 25 articles...\n",
      "[RelevanceScorer] 8 articles passed 0.7 threshold\n",
      "[Pipeline] Sentiment Score: 0.939 (Positive)\n",
      "[ReportGenerator] Report saved to news_attribution_report.txt\n",
      "[ReportGenerator] CSV saved to results_aug7.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = PipelineConfig(\n",
    "        newsapi_key=\"YOURAPIHERE\",\n",
    "        groq_api_key=\"YOURAPIHERE\"\n",
    "    )\n",
    "    \n",
    "    pipeline = NewsAttributionPipeline(config)\n",
    "    results = pipeline.run()\n",
    "    \n",
    "    ReportGenerator.save_report(results, \"news_attribution_report.txt\")\n",
    "    ReportGenerator.save_csv(results, \"results_aug7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e77b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
